"""
Script th·ª±c nghi·ªám ƒë·ªÉ ch·∫°y Baseline v√† Advanced model nhi·ªÅu l·∫ßn
v√† thu th·∫≠p th·ªëng k√™ cho b√°o c√°o
"""
import sys
import time
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import pandas as pd
import os
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'  # H·ªó tr·ª£ Unicode

# Import c√°c module
import baseline
import advanced

# Python 3 ƒë√£ h·ªó tr·ª£ UTF-8 m·∫∑c ƒë·ªãnh, kh√¥ng c·∫ßn thi·∫øt l·∫≠p l·∫°i


def calculate_makespan(csp_result) -> float:
    """T√≠nh th·ªùi gian ho√†n th√†nh d·ª± √°n (ng√†y)"""
    if not csp_result.assignment:
        return 0.0
    
    max_end_time = None
    for task_id, assignment in csp_result.assignment.items():
        task = next(t for t in csp_result.cac_tacvu if t.id == task_id)
        end_time = assignment.start_time + timedelta(hours=task.duration)
        if max_end_time is None or end_time > max_end_time:
            max_end_time = end_time
    
    makespan = max_end_time - csp_result.project_start_date
    return makespan.total_seconds() / 86400  # Tr·∫£ v·ªÅ s·ªë ng√†y


def calculate_constraint_satisfaction(csp_result) -> float:
    """
    T√≠nh % r√†ng bu·ªôc th·ªèa m√£n
    Ki·ªÉm tra t·∫•t c·∫£ r√†ng bu·ªôc c·ª©ng: k·ªπ nƒÉng, ph·ª• thu·ªôc, deadline, s·ª©c ch·ª©a, khung gi·ªù
    """
    if not csp_result.assignment:
        return 0.0
    
    total_constraints = 0
    satisfied_constraints = 0
    
    for task_id, assignment in csp_result.assignment.items():
        task = next(t for t in csp_result.cac_tacvu if t.id == task_id)
        start_time = assignment.start_time
        end_time = start_time + timedelta(hours=task.duration)
        
        # 1. R√†ng bu·ªôc k·ªπ nƒÉng
        total_constraints += 1
        if task.required_skill in assignment.nhansu.skills:
            satisfied_constraints += 1
        
        # 2. R√†ng bu·ªôc ph·ª• thu·ªôc
        for dep_id in task.dependencies:
            total_constraints += 1
            if dep_id in csp_result.assignment:
                dep_assignment = csp_result.assignment[dep_id]
                dep_task = next(t for t in csp_result.cac_tacvu if t.id == dep_id)
                dep_end = dep_assignment.start_time + timedelta(hours=dep_task.duration)
                if start_time >= dep_end:
                    satisfied_constraints += 1
        
        # 3. R√†ng bu·ªôc deadline
        total_constraints += 1
        deadline = csp_result.project_start_date + timedelta(days=task.deadline)
        if end_time <= deadline:
            satisfied_constraints += 1
        
        # 4. R√†ng bu·ªôc khung th·ªùi gian d·ª± √°n
        total_constraints += 1
        if (start_time >= csp_result.project_start_date and 
            end_time <= csp_result.project_end_date):
            satisfied_constraints += 1
        
        # 5. R√†ng bu·ªôc khung gi·ªù l√†m vi·ªác (8h-17h)
        total_constraints += 1
        if (start_time.hour >= 8 and start_time.hour < 17 and
            end_time.hour >= 8 and end_time.hour <= 17):
            satisfied_constraints += 1
    
    # 6. R√†ng bu·ªôc s·ª©c ch·ª©a (ki·ªÉm tra m·ªói nh√¢n vi√™n m·ªói ng√†y)
    for nhansu in csp_result.cac_nhansu:
        # Nh√≥m c√¥ng vi·ªác theo ng√†y
        daily_workload = {}
        for task_id, assignment in csp_result.assignment.items():
            if assignment.nhansu.id == nhansu.id:
                task = next(t for t in csp_result.cac_tacvu if t.id == task_id)
                work_date = assignment.start_time.date()
                if work_date not in daily_workload:
                    daily_workload[work_date] = 0
                daily_workload[work_date] += task.duration
        
        # Ki·ªÉm tra m·ªói ng√†y
        for work_date, total_hours in daily_workload.items():
            total_constraints += 1
            if total_hours <= nhansu.daily_capacity:
                satisfied_constraints += 1
    
    return (satisfied_constraints / total_constraints * 100.0) if total_constraints > 0 else 0.0


def calculate_workload_std_dev(csp_result) -> float:
    """
    T√≠nh ƒë·ªô l·ªách chu·∫©n workload (Standard Deviation)
    ƒêo s·ª± ph√¢n b·ªë c√¥ng vi·ªác gi·ªØa c√°c nh√¢n vi√™n
    """
    if not csp_result.assignment:
        return 0.0
    
    # T√≠nh workload cho m·ªói nh√¢n vi√™n
    workloads = []
    for nhansu in csp_result.cac_nhansu:
        total_hours = 0
        for task_id, assignment in csp_result.assignment.items():
            if assignment.nhansu.id == nhansu.id:
                task = next(t for t in csp_result.cac_tacvu if t.id == task_id)
                total_hours += task.duration
        workloads.append(total_hours)
    
    if not workloads:
        return 0.0
    
    # T√≠nh ƒë·ªô l·ªách chu·∫©n
    if len(workloads) > 1:
        std_dev = statistics.stdev(workloads)
    else:
        std_dev = 0.0
    
    return std_dev


def calculate_load_balance_score(csp_result) -> float:
    """
    T√≠nh ƒëi·ªÉm c√¢n b·∫±ng t·∫£i (Load Balance Score)
    ƒêi·ªÉm c√†ng cao = c√¢n b·∫±ng c√†ng t·ªët
    """
    if not csp_result.assignment:
        return 0.0
    
    # T√≠nh workload cho m·ªói nh√¢n vi√™n
    workloads = []
    for nhansu in csp_result.cac_nhansu:
        total_hours = 0
        for task_id, assignment in csp_result.assignment.items():
            if assignment.nhansu.id == nhansu.id:
                task = next(t for t in csp_result.cac_tacvu if t.id == task_id)
                total_hours += task.duration
        workloads.append(total_hours)
    
    if not workloads:
        return 0.0
    
    # T√≠nh ƒë·ªô l·ªách chu·∫©n
    if len(workloads) > 1:
        std_dev = statistics.stdev(workloads)
    else:
        std_dev = 0.0
    
    # Chuy·ªÉn ƒë·ªïi th√†nh ƒëi·ªÉm: ƒë·ªô l·ªách c√†ng nh·ªè = ƒëi·ªÉm c√†ng cao
    # C√¥ng th·ª©c: score = 1 / (1 + std_dev)
    score = 1.0 / (1.0 + std_dev)
    return score


def calculate_priority_score(csp_result) -> float:
    """
    T√≠nh ƒëi·ªÉm ∆∞u ti√™n (Priority Score)
    ∆Øu ti√™n c√°c t√°c v·ª• c√≥ ƒë·ªô ∆∞u ti√™n cao ƒë∆∞·ª£c th·ª±c hi·ªán s·ªõm
    """
    if not csp_result.assignment:
        return 0.0
    
    total_score = 0.0
    project_duration = (csp_result.project_end_date - csp_result.project_start_date).total_seconds()
    
    for task_id, assignment in csp_result.assignment.items():
        task = next(t for t in csp_result.cac_tacvu if t.id == task_id)
        
        # T√≠nh th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu chu·∫©n h√≥a (0 = b·∫Øt ƒë·∫ßu d·ª± √°n, 1 = k·∫øt th√∫c d·ª± √°n)
        time_elapsed = (assignment.start_time - csp_result.project_start_date).total_seconds()
        normalized_time = time_elapsed / project_duration if project_duration > 0 else 0
        
        # T√°c v·ª• ∆∞u ti√™n cao th·ª±c hi·ªán s·ªõm ‚Üí ƒëi·ªÉm cao
        task_score = task.priority * (1.0 - normalized_time)
        total_score += task_score
    
    # Chu·∫©n h√≥a ƒëi·ªÉm (chia cho t·ªïng priority c·ªßa t·∫•t c·∫£ t√°c v·ª•)
    total_priority = sum(t.priority for t in csp_result.cac_tacvu)
    normalized_score = total_score / total_priority if total_priority > 0 else 0
    
    return normalized_score


def run_single_experiment(dataset_folder: str, project_start_date: datetime, 
                         project_end_date: datetime, model_name: str) -> Dict:
    """
    Ch·∫°y m·ªôt l·∫ßn th·ª±c nghi·ªám v·ªõi m·ªôt m√¥ h√¨nh
    
    Returns:
        Dictionary ch·ª©a c√°c metrics
    """
    start_time = time.time()
    
    if model_name == "Baseline":
        result = baseline.solve_csp(dataset_folder, project_start_date, project_end_date)
        backtrack_count = 0  # Baseline kh√¥ng track
        ac3_pruned = 0
        fc_pruned = 0
    else:  # Advanced
        result = advanced.solve_csp(dataset_folder, project_start_date, project_end_date)
        backtrack_count = result.backtrack_count
        ac3_pruned = result.ac3_pruned_count
        fc_pruned = result.fc_pruned_count
    
    runtime = time.time() - start_time
    
    # T√≠nh c√°c metrics
    makespan = calculate_makespan(result) if result.solution_found else 0.0
    constraint_satisfaction = calculate_constraint_satisfaction(result) if result.solution_found else 0.0
    workload_std = calculate_workload_std_dev(result) if result.solution_found else 0.0
    
    # T√≠nh r√†ng bu·ªôc m·ªÅm (ch·ªâ cho Advanced khi c√≥ solution)
    load_balance_score = 0.0
    priority_score = 0.0
    if result.solution_found:
        load_balance_score = calculate_load_balance_score(result)
        priority_score = calculate_priority_score(result)
    
    return {
        'runtime': runtime,
        'makespan': makespan,
        'constraint_satisfaction': constraint_satisfaction,
        'workload_std_dev': workload_std,
        'solution_found': result.solution_found,
        'backtrack_count': backtrack_count,
        'ac3_pruned': ac3_pruned,
        'fc_pruned': fc_pruned,
        'load_balance_score': load_balance_score,
        'priority_score': priority_score
    }


def run_experiments(dataset_folder: str, num_runs: int = 1) -> Tuple[List[Dict], List[Dict]]:
    """
    Ch·∫°y th·ª±c nghi·ªám nhi·ªÅu l·∫ßn cho c·∫£ Baseline v√† Advanced
    
    Args:
        dataset_folder: ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset
        num_runs: S·ªë l·∫ßn ch·∫°y (m·∫∑c ƒë·ªãnh 15)
    
    Returns:
        Tuple (baseline_results, advanced_results)
    """
    # Thi·∫øt l·∫≠p th·ªùi gian d·ª± √°n
    project_start_date = datetime(2024, 1, 1, 8, 0, 0)  # 01/01/2024 08:00
    project_end_date = datetime(2024, 1, 31, 17, 0, 0)  # 31/01/2024 17:00
    
    baseline_results = []
    advanced_results = []
    
    print("=" * 70)
    print("B·∫ÆT ƒê·∫¶U TH·ª∞C NGHI·ªÜM")
    print("=" * 70)
    print(f"Dataset: {dataset_folder}")
    print(f"S·ªë l·∫ßn ch·∫°y: {num_runs}")
    if num_runs == 1:
        print("L∆∞u √Ω: CSP l√† deterministic ‚Üí k·∫øt qu·∫£ gi·ªëng nhau m·ªói l·∫ßn ch·∫°y")
        print("       Ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn ƒë·ªÉ l·∫•y k·∫øt qu·∫£.")
    else:
        print("L∆∞u √Ω: Ch·∫°y nhi·ªÅu l·∫ßn ch·ªâ ƒë·ªÉ t√≠nh trung b√¨nh runtime (c√≥ th·ªÉ dao ƒë·ªông).")
    print(f"Th·ªùi gian d·ª± √°n: {project_start_date.strftime('%d/%m/%Y')} - {project_end_date.strftime('%d/%m/%Y')}")
    print("=" * 70)
    print()
    
    # Ch·∫°y Baseline
    print("ƒêang ch·∫°y Baseline model...")
    for i in range(num_runs):
        print(f"  L·∫ßn {i+1}/{num_runs}...", end=" ", flush=True)
        result = run_single_experiment(dataset_folder, project_start_date, project_end_date, "Baseline")
        baseline_results.append(result)
        status = "‚úì" if result['solution_found'] else "‚úó"
        print(f"{status} ({result['runtime']:.4f}s)")
    
    print()
    
    # Ch·∫°y Advanced
    print("ƒêang ch·∫°y Advanced model...")
    for i in range(num_runs):
        print(f"  L·∫ßn {i+1}/{num_runs}...", end=" ", flush=True)
        result = run_single_experiment(dataset_folder, project_start_date, project_end_date, "Advanced")
        advanced_results.append(result)
        status = "‚úì" if result['solution_found'] else "‚úó"
        print(f"{status} ({result['runtime']:.4f}s)")
    
    print()
    return baseline_results, advanced_results


def calculate_statistics(results: List[Dict]) -> Dict:
    """
    T√≠nh to√°n th·ªëng k√™ t·ª´ danh s√°ch k·∫øt qu·∫£
    
    Returns:
        Dictionary ch·ª©a mean, std_dev, min, max cho m·ªói metric
    """
    if not results:
        return {}
    
    stats = {}
    
    # L·ªçc c√°c metrics s·ªë
    metrics = ['runtime', 'makespan', 'constraint_satisfaction', 'workload_std_dev', 
               'backtrack_count', 'ac3_pruned', 'fc_pruned', 'load_balance_score', 'priority_score']
    
    for metric in metrics:
        values = [r[metric] for r in results if metric in r]
        if values:
            stats[f'{metric}_mean'] = statistics.mean(values)
            stats[f'{metric}_std'] = statistics.stdev(values) if len(values) > 1 else 0.0
            stats[f'{metric}_min'] = min(values)
            stats[f'{metric}_max'] = max(values)
    
    # Success rate
    success_count = sum(1 for r in results if r.get('solution_found', False))
    stats['success_rate'] = (success_count / len(results)) * 100.0
    stats['success_count'] = success_count
    stats['total_runs'] = len(results)
    
    return stats


def export_to_excel(baseline_stats: Dict, advanced_stats: Dict, 
                    baseline_results: List[Dict], advanced_results: List[Dict],
                    output_file: str = "experiment_results.xlsx"):
    """
    Xu·∫•t k·∫øt qu·∫£ ra file Excel v·ªõi nhi·ªÅu sheet
    """
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # Sheet 1: T·ªïng quan th·ªëng k√™
        summary_data = {
            'Metric': [
                'Th·ªùi gian ch·∫°y (s)',
                'Makespan (ng√†y)',
                '% R√†ng bu·ªôc th·ªèa m√£n',
                'ƒê·ªô l·ªách chu·∫©n Workload',
                'Load Balance Score',
                'Priority Score',
                'S·ªë l·∫ßn Backtrack',
                'AC-3 Pruned',
                'Forward Checking Pruned',
                'Success Rate (%)'
            ],
            'Baseline (Mean ¬± Std)': [
                f"{baseline_stats.get('runtime_mean', 0):.4f} ¬± {baseline_stats.get('runtime_std', 0):.4f}",
                f"{baseline_stats.get('makespan_mean', 0):.2f} ¬± {baseline_stats.get('makespan_std', 0):.2f}",
                f"{baseline_stats.get('constraint_satisfaction_mean', 0):.2f} ¬± {baseline_stats.get('constraint_satisfaction_std', 0):.2f}",
                f"{baseline_stats.get('workload_std_dev_mean', 0):.2f} ¬± {baseline_stats.get('workload_std_dev_std', 0):.2f}",
                f"{baseline_stats.get('load_balance_score_mean', 0):.4f} ¬± {baseline_stats.get('load_balance_score_std', 0):.4f}",
                f"{baseline_stats.get('priority_score_mean', 0):.4f} ¬± {baseline_stats.get('priority_score_std', 0):.4f}",
                f"{baseline_stats.get('backtrack_count_mean', 0):.0f} ¬± {baseline_stats.get('backtrack_count_std', 0):.0f}",
                f"{baseline_stats.get('ac3_pruned_mean', 0):.0f} ¬± {baseline_stats.get('ac3_pruned_std', 0):.0f}",
                f"{baseline_stats.get('fc_pruned_mean', 0):.0f} ¬± {baseline_stats.get('fc_pruned_std', 0):.0f}",
                f"{baseline_stats.get('success_rate', 0):.1f}%"
            ],
            'Advanced (Mean ¬± Std)': [
                f"{advanced_stats.get('runtime_mean', 0):.4f} ¬± {advanced_stats.get('runtime_std', 0):.4f}",
                f"{advanced_stats.get('makespan_mean', 0):.2f} ¬± {advanced_stats.get('makespan_std', 0):.2f}",
                f"{advanced_stats.get('constraint_satisfaction_mean', 0):.2f} ¬± {advanced_stats.get('constraint_satisfaction_std', 0):.2f}",
                f"{advanced_stats.get('workload_std_dev_mean', 0):.2f} ¬± {advanced_stats.get('workload_std_dev_std', 0):.2f}",
                f"{advanced_stats.get('load_balance_score_mean', 0):.4f} ¬± {advanced_stats.get('load_balance_score_std', 0):.4f}",
                f"{advanced_stats.get('priority_score_mean', 0):.4f} ¬± {advanced_stats.get('priority_score_std', 0):.4f}",
                f"{advanced_stats.get('backtrack_count_mean', 0):.0f} ¬± {advanced_stats.get('backtrack_count_std', 0):.0f}",
                f"{advanced_stats.get('ac3_pruned_mean', 0):.0f} ¬± {advanced_stats.get('ac3_pruned_std', 0):.0f}",
                f"{advanced_stats.get('fc_pruned_mean', 0):.0f} ¬± {advanced_stats.get('fc_pruned_std', 0):.0f}",
                f"{advanced_stats.get('success_rate', 0):.1f}%"
            ],
            'C·∫£i thi·ªán': [
                f"{(1 - advanced_stats.get('runtime_mean', 1) / max(baseline_stats.get('runtime_mean', 1), 0.0001)) * 100:.1f}%" if baseline_stats.get('runtime_mean', 0) > 0 else "N/A",
                f"{(1 - advanced_stats.get('makespan_mean', 1) / max(baseline_stats.get('makespan_mean', 1), 0.0001)) * 100:.1f}%" if baseline_stats.get('makespan_mean', 0) > 0 else "N/A",
                f"{(advanced_stats.get('constraint_satisfaction_mean', 0) - baseline_stats.get('constraint_satisfaction_mean', 0)):.1f}%" if baseline_stats.get('constraint_satisfaction_mean', 0) > 0 else "N/A",
                f"{(1 - advanced_stats.get('workload_std_dev_mean', 1) / max(baseline_stats.get('workload_std_dev_mean', 1), 0.0001)) * 100:.1f}%" if baseline_stats.get('workload_std_dev_mean', 0) > 0 else "N/A",
                f"{(advanced_stats.get('load_balance_score_mean', 0) - baseline_stats.get('load_balance_score_mean', 0)):.4f}" if baseline_stats.get('load_balance_score_mean', 0) > 0 else "N/A",
                f"{(advanced_stats.get('priority_score_mean', 0) - baseline_stats.get('priority_score_mean', 0)):.4f}" if baseline_stats.get('priority_score_mean', 0) > 0 else "N/A",
                "N/A",
                "N/A",
                "N/A",
                f"{(advanced_stats.get('success_rate', 0) - baseline_stats.get('success_rate', 0)):.1f}%" if baseline_stats.get('success_rate', 0) > 0 else "N/A"
            ]
        }
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_excel(writer, sheet_name='T·ªïng quan', index=False)
        
        # Sheet 2: Chi ti·∫øt Baseline
        df_baseline = pd.DataFrame(baseline_results)
        df_baseline.to_excel(writer, sheet_name='Baseline Chi ti·∫øt', index=False)
        
        # Sheet 3: Chi ti·∫øt Advanced
        df_advanced = pd.DataFrame(advanced_results)
        df_advanced.to_excel(writer, sheet_name='Advanced Chi ti·∫øt', index=False)
        
        # Sheet 4: Th·ªëng k√™ Baseline
        baseline_stats_df = pd.DataFrame([baseline_stats]).T
        baseline_stats_df.columns = ['Gi√° tr·ªã']
        baseline_stats_df.to_excel(writer, sheet_name='Baseline Th·ªëng k√™')
        
        # Sheet 5: Th·ªëng k√™ Advanced
        advanced_stats_df = pd.DataFrame([advanced_stats]).T
        advanced_stats_df.columns = ['Gi√° tr·ªã']
        advanced_stats_df.to_excel(writer, sheet_name='Advanced Th·ªëng k√™')
    
    print(f"\n‚úì K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c xu·∫•t ra file: {output_file}")


def print_summary(baseline_stats: Dict, advanced_stats: Dict):
    """In t√≥m t·∫Øt k·∫øt qu·∫£ ra console"""
    print("\n" + "=" * 70)
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢ TH·ª∞C NGHI·ªÜM")
    print("=" * 70)
    
    print("\nüìä BASELINE MODEL:")
    print(f"  Th·ªùi gian ch·∫°y:     {baseline_stats.get('runtime_mean', 0):.4f} ¬± {baseline_stats.get('runtime_std', 0):.4f} gi√¢y")
    print(f"  Makespan:           {baseline_stats.get('makespan_mean', 0):.2f} ¬± {baseline_stats.get('makespan_std', 0):.2f} ng√†y")
    print(f"  % R√†ng bu·ªôc:        {baseline_stats.get('constraint_satisfaction_mean', 0):.2f} ¬± {baseline_stats.get('constraint_satisfaction_std', 0):.2f}%")
    print(f"  Workload Std Dev:   {baseline_stats.get('workload_std_dev_mean', 0):.2f} ¬± {baseline_stats.get('workload_std_dev_std', 0):.2f}")
    print(f"  Success Rate:       {baseline_stats.get('success_rate', 0):.1f}% ({baseline_stats.get('success_count', 0)}/{baseline_stats.get('total_runs', 0)})")
    
    print("\nüöÄ ADVANCED MODEL:")
    print(f"  Th·ªùi gian ch·∫°y:     {advanced_stats.get('runtime_mean', 0):.4f} ¬± {advanced_stats.get('runtime_std', 0):.4f} gi√¢y")
    print(f"  Makespan:           {advanced_stats.get('makespan_mean', 0):.2f} ¬± {advanced_stats.get('makespan_std', 0):.2f} ng√†y")
    print(f"  % R√†ng bu·ªôc:        {advanced_stats.get('constraint_satisfaction_mean', 0):.2f} ¬± {advanced_stats.get('constraint_satisfaction_std', 0):.2f}%")
    print(f"  Workload Std Dev:   {advanced_stats.get('workload_std_dev_mean', 0):.2f} ¬± {advanced_stats.get('workload_std_dev_std', 0):.2f}")
    print(f"  Load Balance Score: {advanced_stats.get('load_balance_score_mean', 0):.4f} ¬± {advanced_stats.get('load_balance_score_std', 0):.4f}")
    print(f"  Priority Score:     {advanced_stats.get('priority_score_mean', 0):.4f} ¬± {advanced_stats.get('priority_score_std', 0):.4f}")
    print(f"  S·ªë l·∫ßn Backtrack:   {advanced_stats.get('backtrack_count_mean', 0):.0f} ¬± {advanced_stats.get('backtrack_count_std', 0):.0f}")
    print(f"  AC-3 Pruned:         {advanced_stats.get('ac3_pruned_mean', 0):.0f} ¬± {advanced_stats.get('ac3_pruned_std', 0):.0f}")
    print(f"  FC Pruned:           {advanced_stats.get('fc_pruned_mean', 0):.0f} ¬± {advanced_stats.get('fc_pruned_std', 0):.0f}")
    print(f"  Success Rate:       {advanced_stats.get('success_rate', 0):.1f}% ({advanced_stats.get('success_count', 0)}/{advanced_stats.get('total_runs', 0)})")
    
    # T√≠nh c·∫£i thi·ªán
    if baseline_stats.get('runtime_mean', 0) > 0:
        speedup = baseline_stats.get('runtime_mean', 1) / max(advanced_stats.get('runtime_mean', 0.0001), 0.0001)
        print(f"\n‚ö° C·∫¢I THI·ªÜN:")
        print(f"  T·ªëc ƒë·ªô:             {speedup:.2f}x nhanh h∆°n")
        if baseline_stats.get('makespan_mean', 0) > 0:
            makespan_improvement = (1 - advanced_stats.get('makespan_mean', 1) / baseline_stats.get('makespan_mean', 1)) * 100
            print(f"  Makespan:           {makespan_improvement:.1f}% t·ªët h∆°n")
        constraint_improvement = advanced_stats.get('constraint_satisfaction_mean', 0) - baseline_stats.get('constraint_satisfaction_mean', 0)
        print(f"  % R√†ng bu·ªôc:        +{constraint_improvement:.1f}%")
        
        workload_improvement = (1 - advanced_stats.get('workload_std_dev_mean', 1) / max(baseline_stats.get('workload_std_dev_mean', 1), 0.0001)) * 100
        print(f"  Workload c√¢n b·∫±ng:  {workload_improvement:.1f}% t·ªët h∆°n")
    
    # Ph√¢n t√≠ch hi·ªáu qu·∫£ k·ªπ thu·∫≠t (ch·ªâ khi ch·∫°y 1 l·∫ßn ho·∫∑c c√≥ d·ªØ li·ªáu)
    print(f"\nüìà PH√ÇN T√çCH HI·ªÜU QU·∫¢ K·ª∏ THU·∫¨T (Advanced Model):")
    ac3_pruned = advanced_stats.get('ac3_pruned_mean', 0)
    fc_pruned = advanced_stats.get('fc_pruned_mean', 0)
    backtrack = advanced_stats.get('backtrack_count_mean', 0)
    load_balance_score = advanced_stats.get('load_balance_score_mean', 0)
    priority_score = advanced_stats.get('priority_score_mean', 0)
    
    print(f"  AC-3 ƒë√£ c·∫Øt t·ªâa:     {ac3_pruned:.0f} gi√° tr·ªã kh√¥ng kh·∫£ thi")
    print(f"  Forward Checking:    {fc_pruned:.0f} gi√° tr·ªã b·ªã lo·∫°i b·ªè")
    print(f"  S·ªë l·∫ßn Backtrack:    {backtrack:.0f} l·∫ßn")
    print(f"  Load Balance Score:  {load_balance_score:.4f}")
    print(f"  Priority Score:      {priority_score:.4f}")
    
    if backtrack == 0:
        print(f"  ‚Üí T√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i ngay t·ª´ ƒë·∫ßu, kh√¥ng c·∫ßn backtrack!")
    elif backtrack < 5:
        print(f"  ‚Üí R·∫•t √≠t backtrack, thu·∫≠t to√°n hi·ªáu qu·∫£ cao")
    else:
        print(f"  ‚Üí C·∫ßn {backtrack:.0f} l·∫ßn backtrack ƒë·ªÉ t√¨m l·ªùi gi·∫£i")
    
    print("=" * 70)


def plot_runtime_per_trial(baseline_results: List[Dict], advanced_results: List[Dict], 
                           output_file: str = "runtime_per_trial.png"):
    """
    V·∫Ω bi·ªÉu ƒë·ªì line graph cho runtime theo t·ª´ng trial
    """
    trials = list(range(1, len(baseline_results) + 1))
    baseline_runtimes = [r['runtime'] for r in baseline_results]
    advanced_runtimes = [r['runtime'] for r in advanced_results]
    
    plt.figure(figsize=(12, 6))
    plt.plot(trials, baseline_runtimes, 'o-', label='Baseline', color='#1f77b4', linewidth=2, markersize=6)
    plt.plot(trials, advanced_runtimes, 'o-', label='Advanced', color='#ff7f0e', linewidth=2, markersize=6)
    
    plt.xlabel('Trial', fontsize=12, fontweight='bold')
    plt.ylabel('Runtime (seconds)', fontsize=12, fontweight='bold')
    plt.title('Runtime per Trial', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.tight_layout()
    
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_file}")
    plt.close()


def plot_average_comparison(baseline_stats: Dict, advanced_stats: Dict,
                           output_file: str = "average_comparison.png"):
    """
    V·∫Ω bi·ªÉu ƒë·ªì bar chart so s√°nh trung b√¨nh c√°c metrics
    Style gi·ªëng gui_app.py
    """
    fig = plt.figure(figsize=(12, 8))
    
    categories = ['Baseline', 'Advanced']
    colors = ['#e74c3c', '#27ae60']  # ƒê·ªè cho Baseline, Xanh l√° cho Advanced (gi·ªëng gui_app.py)
    
    # 1. Makespan
    ax1 = fig.add_subplot(2, 2, 1)
    makespan_values = [
        baseline_stats.get('makespan_mean', 0),
        advanced_stats.get('makespan_mean', 0)
    ]
    bars1 = ax1.bar(categories, makespan_values, color=colors, alpha=0.7, edgecolor='black')
    ax1.set_ylabel('Ng√†y', fontsize=10)
    ax1.set_title('Th·ªùi Gian Ho√†n Th√†nh D·ª± √Ån\n(Makespan - C√†ng th·∫•p c√†ng t·ªët)', 
                 fontsize=11, fontweight='bold')
    ax1.grid(axis='y', alpha=0.3)
    
    # Th√™m gi√° tr·ªã l√™n c·ªôt
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f} ng√†y', ha='center', va='bottom', fontsize=9)
    
    # 2. % R√†ng Bu·ªôc Th·ªèa
    ax2 = fig.add_subplot(2, 2, 2)
    constraint_values = [
        baseline_stats.get('constraint_satisfaction_mean', 0),
        advanced_stats.get('constraint_satisfaction_mean', 0)
    ]
    bars2 = ax2.bar(categories, constraint_values, color=colors, alpha=0.7, edgecolor='black')
    ax2.set_ylabel('%', fontsize=10)
    ax2.set_title('% R√†ng Bu·ªôc Th·ªèa\n(C√†ng cao c√†ng t·ªët)', 
                 fontsize=11, fontweight='bold')
    ax2.set_ylim([0, 105])
    ax2.grid(axis='y', alpha=0.3)
    
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 3. ƒê·ªô L·ªách Chu·∫©n Workload
    ax3 = fig.add_subplot(2, 2, 3)
    workload_std_values = [
        baseline_stats.get('workload_std_dev_mean', 0),
        advanced_stats.get('workload_std_dev_mean', 0)
    ]
    bars3 = ax3.bar(categories, workload_std_values, color=colors, alpha=0.7, edgecolor='black')
    ax3.set_ylabel('Gi·ªù', fontsize=10)
    ax3.set_title('ƒê·ªô L·ªách Chu·∫©n Workload\n(C√†ng th·∫•p c√†ng t·ªët)', 
                 fontsize=11, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    
    for bar in bars3:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}', ha='center', va='bottom', fontsize=9)
    
    # 4. Th·ªùi Gian Ch·∫°y
    ax4 = fig.add_subplot(2, 2, 4)
    time_values = [
        baseline_stats.get('runtime_mean', 0),
        advanced_stats.get('runtime_mean', 0)
    ]
    bars4 = ax4.bar(categories, time_values, color=colors, alpha=0.7, edgecolor='black')
    ax4.set_ylabel('Gi√¢y', fontsize=10)
    ax4.set_title('Th·ªùi Gian Ch·∫°y\n(C√†ng th·∫•p c√†ng t·ªët)', 
                 fontsize=11, fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)
    
    for bar in bars4:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}s', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_file}")
    plt.close()


def plot_all_charts(baseline_results: List[Dict], advanced_results: List[Dict],
                   baseline_stats: Dict, advanced_stats: Dict):
    """
    V·∫Ω t·∫•t c·∫£ c√°c bi·ªÉu ƒë·ªì
    """
    print("\nƒêang v·∫Ω bi·ªÉu ƒë·ªì...")
    
    # 1. Runtime per trial (line graph) - ch·ªâ v·∫Ω n·∫øu c√≥ nhi·ªÅu h∆°n 1 l·∫ßn ch·∫°y
    if len(baseline_results) > 1:
        plot_runtime_per_trial(baseline_results, advanced_results, "runtime_per_trial.png")
    else:
        print("  (B·ªè qua runtime_per_trial v√¨ ch·ªâ ch·∫°y 1 l·∫ßn)")
    
    # 2. Average comparison (bar chart)
    plot_average_comparison(baseline_stats, advanced_stats, "average_comparison.png")
    
    print("‚úì Ho√†n th√†nh v·∫Ω bi·ªÉu ƒë·ªì!")


def main():
    """H√†m ch√≠nh"""
    print("=" * 70)
    print("H·ªÜ TH·ªêNG TH·ª∞C NGHI·ªÜM - SO S√ÅNH BASELINE V√Ä ADVANCED MODEL")
    print("=" * 70)
    print()
    
    # Ch·ªçn dataset
    dataset_folder = "datasets/medium_project"
    
    # S·ªë l·∫ßn ch·∫°y - CSP l√† deterministic n√™n k·∫øt qu·∫£ gi·ªëng nhau
    # Ch·ªâ c·∫ßn ch·∫°y nhi·ªÅu l·∫ßn n·∫øu mu·ªën t√≠nh trung b√¨nh runtime (c√≥ th·ªÉ dao ƒë·ªông do h·ªá th·ªëng)
    # M·∫∑c ƒë·ªãnh ch·∫°y 1 l·∫ßn v√¨ k·∫øt qu·∫£ gi·ªëng nhau
    num_runs = 1
    
    print(f"Dataset: {dataset_folder}")
    print(f"S·ªë l·∫ßn ch·∫°y m·ªói m√¥ h√¨nh: {num_runs}")
    print("L∆∞u √Ω: CSP l√† deterministic, k·∫øt qu·∫£ s·∫Ω gi·ªëng nhau m·ªói l·∫ßn ch·∫°y.")
    print("       Ch·ªâ c·∫ßn ch·∫°y nhi·ªÅu l·∫ßn n·∫øu mu·ªën t√≠nh trung b√¨nh runtime.")
    print()
    
    # Ch·∫°y th·ª±c nghi·ªám
    baseline_results, advanced_results = run_experiments(dataset_folder, num_runs)
    
    # T√≠nh to√°n th·ªëng k√™
    print("ƒêang t√≠nh to√°n th·ªëng k√™...")
    baseline_stats = calculate_statistics(baseline_results)
    advanced_stats = calculate_statistics(advanced_results)
    
    # In t√≥m t·∫Øt
    print_summary(baseline_stats, advanced_stats)
    
    # Xu·∫•t ra Excel
    output_file = "experiment_results_medium_project.xlsx"
    export_to_excel(baseline_stats, advanced_stats, baseline_results, advanced_results, output_file)
    
    # V·∫Ω bi·ªÉu ƒë·ªì
    plot_all_charts(baseline_results, advanced_results, baseline_stats, advanced_stats)
    
    print("\n‚úì Ho√†n th√†nh th·ª±c nghi·ªám!")


if __name__ == "__main__":
    main()

